# 绪论
## 1.1 引言
> 假设使用P来估计计算机程序在某任务类T上的性能，若一个程序通过利用经验E在T中任务上获得了性能改善，则我们就说关于T和P，该程序对E进行了学习。

机器学习致力于研究如何通过计算的手段，利用经验来改善系统自身的性能。因为“经验”通常以数据的方式储存，所以机器学习研究的就是关于在计算机上从数据中产生模型的算法，即“学习算法”。


## 1.2 基本术语
### 基本定义
* 数据集 data set 一组数据的集合
* 示例/样本 instance/sample 数据集中的每一条记录
* 属性/特征 attribute/feature 反映事件或对象在某方面的表现或性质的事项
* 属性值 attribute value 属性上的取值
* 属性空间/样本空间 attribute space/sample space 属性张成的空间
* 特征向量 feature vector 一个示例中的所有属性可以对应与样本空间中的一个点，空间中的每一个点都对应一个坐标向量，因此也可以把一个示例称为一个特征向量
* 维数 dimensionality 一个示例由d个属性描述
* 训练数据/训练样本 training data/training sample
* 预测 prediction 
* 标记/样例/标记空间 label/example/example sapce 输出
### 类别

* 监督学习 supervised learning 训练数据有标记信息 
* * 分类 classification 欲预测的是离散值
* * 回归 regression 欲预测的是连续值

* 无监督学习 unsupervised learning 训练数据无标记信息
* * 聚类 clustering 将训练集中的分为若干cluster

## 1.3 归纳空间
我们可以把学习过程看作一个在所有假设组成的空间中进行搜索的过程，搜索目标是找到与训练集匹配的假设，假设的表示一旦确定，假设空间及其规模大小就确定了。

我们可以选择一个策略对这个假设空间进行搜索，在搜索过程中不断删除与正例不一致的假设、或与反例一致的假设，最终获得与训练集一致的假设(hypothesis)，这就是我们最后学习的结果。

需要注意的是，显示问题中常常面临很大的假设空间，但是学习过程是基于有限样本训练集进行的。因此可能有多个假设与训练集一直，即存在一个与训练集一致的“假设集合”，我们称之为 版本空间 (version space).

## 1.4 归纳偏好
在一个版本空间中有多个假设可以选择，这些假设面对已经检验过的数据集会有相同的预测，但是如果面对一个新的数据集则会有不同的预测，那么我们应该选择哪一个模型呢？

> 这里有点像函数逼近和函数插值的区别

对于一个具体的学习算法，必须产生一个模型。这时算法本身的偏好就会起到了关键作用：可以选择“尽可能特殊”或者“尽可能一般”的模型。

> 算法选择：奥卡姆剃刀原则主张选择与经验观察一致的最简单假设

在具体的显示问题中，算法的归纳偏好是否与问题本身相匹配，大多数时候直接决定了算法能否取得好的性能。

讨论学习算法的优劣，必须针对具体的学习问题。

## 1.5 发展历程

## 1.6 应用现状



