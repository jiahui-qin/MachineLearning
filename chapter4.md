# 第4章 决策树

## 4.1 基本流程

*p73*

    输入：训练集D，属性集A
    过程：TreeGenerate(D, A)
    生成节点node
    if D 中样本全属于同一类别C then
        将node标记为C类叶节点；return
    end if
    if A为空，或者D中样本在A上取值相同 then
        将node标记为叶节点，其类别标记为D中样本数最多的类；return
    end if

    从A中选择最有划分属性a*
    for a*的每一个值a*v do：
        从node生成一个分支，令Dv表示D在a*上取值为a*v的样本子集
        if Dv为空 then
            将分支节点标记为叶节点，其类别标记为D中样本最多的类，return

        else
            以TreeGenerate(Dv, A\{a*})为分支节点
        end if
    end for
    输出：以node为根节点的一颗决策树

## 4.2 划分选择

### 4.2.1 信息增益

信息熵(information entropy)度量了样本集合纯度 p74，(4.1)
信息增益(information gain)描述用属性a对样本进行划分的效果(4.2)
参考一下信息增益中决策树的构建

### 4.2.2 增益率

信息增益对可取值较多的属性有所偏好，C4.5决策树算法使用增益率(gain ratio)来选择最优划分属性，就是用Gain除以了一个属性a的固有值(4.4)

### 4.2.3 基尼指数

反应了从数据集D中随机抽取两个样本，其类别标记不一致的概率，越小纯度越高：应用的时候也要加一个惩罚项

## 4.3 剪枝处理

剪枝(pruning)：防止过拟合。决策树剪枝的基本策略有预剪枝和后剪枝。预剪枝是指在决策树生成过程，对每隔节点在划分前进行估计，若当前结点的划分不能带来泛化性能的提升，则停止划分当前结点标记为叶节点；后剪枝则是先从训练集生成一颗完整的决策树，然后自底而上的对非叶节点进行考察，若将该节点对应的子树替换为叶节点能带来性能的提升，则将这个子树替换为叶节点。预剪枝有可能造成欠拟合，后剪枝则时间开销比较大

## 4.4 连续与缺失值

连续属性可以使用二分法进行处理，这个划分点可以使增益率最大来确定；缺失值的处理是使用在属性a上没有缺失值的样本计算，将增益率公式推广值有缺失值的情况

## 4.5 多变量决策树

决策树:轴平行(决策树的分类边界有若干个与坐标轴平行的分段组成)p90，图4.11，预测时间开销比较大。

多变量决策树中非叶节点不再是仅对每个属性，而是对属性的线性组合进行测试：为属性加一个权重做一个属性的组合。多变量决策树试图建立一个合适的线性划分器