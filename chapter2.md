# 第2章 模型评估与选择

## 2.1 经验误差与过拟合
* 错误率 error rate 分类错误的样本数占样本总数的比例
* 精度 accuracy 1-错误率
* 训练误差/经验误差 training error/empirical error 学习器在训练集上的误差
* 泛化误差 generalization error 在新样本上的误差

我们希望得到泛化误差比较小的学习器，为了达到这一目的，我们应该从训练样本中尽可能学出适用于所有潜在样本的“普遍规律”，但是可能会导致学习器将训练样本自身的一些特点也当作了所有潜在样本都会具有的一般性质，这样就会导致泛化性能下降，这种现象在机器学习中称为 __过拟合__ ，与过拟合相对应的是欠拟合，指对训练样本的一般性质尚未学好
欠拟合比较容易克服，比如在决策树学习中扩展分支，在神经网络学习中增加训练轮数等，而过拟合则比较麻烦。过拟合是机器学习面临的关键障碍，过拟合是无法避免的，我们所做的只有减小其风险
> 与NP难问题有关？

面对一个问题，我们有多种学习算法可以选择，但是我们选择模型的时候应该如何选择呢？最理想的当然是泛化误差越小越好，但是我们无法直接获得泛化误差，而训练误差又存在过拟合而不适合作为标准。

## 2.2 评估方法
我们可以使用一个**测试集**来测试学习器对新样本的判别能力，然后以测试集的**测试误差**来作为泛化误差的近似。测试样本应尽量不在训练集中出现、未在训练过程中使用过

### 2.2.1 留出法
留出发直接将数据集D划分为两个互斥的集合，其中一个集合作为训练集S，另外一个作为T。

需要注意的是训练集/测试集的划分应尽量保持数据分布的一致性。比如在分类任务中保持样本的类别比例相似。

可以选择多次随机划分，重复进行实验评估后取平均值作为留出法的评估结果

若令训练集S中包含绝大多数样本，则训练出的模型可能更接近与用D训练出的模型，但由于T比较小，评估结果可能不够稳定准确。但是若令测试集T多包含一些样本，被评估的模型与用D训练出的模型可能有较大差别，从而降低了评估结果的保真性。
> 测试集小时，评估结果的方差较大，训练集小时，评估结果的偏差较大

### 2.2.2 交叉验证法
交叉验证法(cross validation)先将数据集D划分为k个大小相似的互斥子集，每个子集都尽可能保持数据分布的一致性，然后每次用k-1个子集作为训练集，剩下的那个子集作为测试集，最终返回k个测试结果的均值。一般采用k=10  （10折交叉验证），也可以采用不同的划分重复p次(p次k折交叉验证).

设数据集中包含m个样本，令k=m，则得到了留一法(Leave one out ,LOO),这种方法的评估结果较为准确，但是训练m个模型的计算开销可能难以忍受。

### 2.2.3 自助法
给定一个包含m个样本的数据集D，每次随机从D中取出一个样本的拷贝放入训练集中，重复m次，显然D中有一部分样本会多次出现，有一部分样本则不出现，这一部分大约占1/3，我们可以将这一部分作为测试集。

这种方法使用于样本量较小/难以有效划分训练、测试集时比较有用，但是在样本数量足够时，留出法和交叉验证法更常用一些

### 2.2.4 调参与最终模型
进行模型评估与选择时，除了要对适用学习算法进行选择，还需要对算法参数进行设定，这就是常说的调参。

现实中常用的调参方法是给每一个参数选定一个范围和变化步长。

在模型选择完成后，学习算法和参数配置都已经选定，此时应该用数据集D重新训练模型，这个模型在训练过程中使用了所有m个样本，这才是我们最终提交给用户的模型。


## 2.3 性能度量
对学习器的泛化性能进行评估：
回归任务最常用的性能度量是**均方误差** p29

### 2.3.1 错误率与精度
错误率：分类错误的样本占样本总数的比例

精度：分类正确的样本数占样本总数的比例

p29 对数据分布D和概率密度函数，错误率和精度的描述

### 2.3.2 查准率、查全率与F1
分类结果混淆矩阵：

| | 预测结果-正确 | 预测结果-错误 | 
| --- |:----:| ---:| 
|真实正例| TP | FN | 
|真实反例| FP | TN |

* 查准率： P=TP/(TP+FP) 选择出来的有多少是正确的

* 查全率： R=TP/(TP+FN) 所有正例有多少被挑了出来

查准率和查全率是一对矛盾的度量：二者不可能同时变高。通常我们可以根据学习器的预测结果对样例进行排序，越靠前学习器就认为是越有可能。按照这个排序，再与实际的结果对比，我们可以计算出针对每一个阈值的查全率、查准率。以查准率为纵轴，查全率为横轴作图，就可以得到查准率-查全率曲线(P-R曲线)。

如果一个学习器的P-R图完全被另外一个学习器包住，则可以认为后者的性能优于前者。如果两个学习器的P-R曲线相交的话，可以计算曲线下面的面积，面积大的表征学习器在查准率和查全率更容易“双高”

平衡点（BEP） 学习器=查全率时的取值，越大越好

* F1:1/F1=1/2*(1/P+1/R)
* 对查全率或者查准率加权： 1/Fβ=1/(1+β^2)*(1/P+β^2/R)

β大于1时查全率有更大影响，β小于1时查准率有更大影响

进行多次训练或者在多个数据集上进行训练时，会得到多个混淆矩阵，在n个混淆矩阵上综合考察查准率和查全率：

* 分别在每一个混淆矩阵上计算出查准率和查全率，然后再计算平均值，得到的叫做宏查准率/宏查全率/宏F1

* 先将各混淆矩阵的对应元素进行平均，得到TP\FP\TN\FN的平均值，再基于平均值算出微查准率/微查全率/微F1

> 以上是描述一个确定的学习器的量

### 2.3.3 ROC与AUC
很多学习器是为测试样本产生一个概率预测，将这个概率预测与一个分类阈值进行比较，大于为正类，小于为反类。这个阈值相当于截断点。不同的应用任务中，我们可以根据任务需要采用不同的截断点。

因此排序本身的质量好坏，体现了综合考虑学习器在不同任务下的“期望泛化性能”的好坏

ROC： 按照学习器的预测结果对样本排序，按照顺序逐个把样本作为正例进行预测，分别计算：
* 横轴 FPR = FP / (TN + FP) 假正确率
* 纵轴 TPR = TP / (TP + FN) 真正确率

然后画出ROC曲线，比较两个学习器的性能可以通过计算ROC曲线下的面积**AUC**来比较

现实中的样本是有限的，所以画出的ROC曲线不是光滑的

> p35页给出了AUC的两种计算方法

### 2.3.4 代价敏感错误率与代价曲线

由于对不同类型错误的敏感不同，可以为错误赋予“非均等代价”

> p36 代价曲线的画法

代价曲线所围城的面积就是所有条件下学习器的期望总体代价

## 2.4 比较检验
用适当的方法对学习器的性能进行比较：

统计假设检验：统计意义上比较好  

本节默认以错误率为性能度量，用ε表示

### 2.4.1 假设检验

根据测试错误率来估算出泛化错误率的分布

对于一次测试用 二项检验

对于多次测试用 t检验

### 2.4.2 交叉验证

k折交叉验证法： 相同的训练集上得到的结果可以用 “成对t检验(paired t-tests)”来进行检验 

### 2.4.3 McNemar检验

两个学习器性能相同，那么可以构造一个卡方统计量

### 2.4.4 Friedman test and Nemenyi 后续检验

Friedman检验可以检验多个数据集的结果，原假设：所有算法性能相同

Nemenyi检验可以用来区分各算法，确定各个算法有没有显著区别。Tukey分布

## 2.5 偏差与方差

偏差-方差分解 解释学习算法泛化性能的一种工具，试图对学习算法的期望泛化误差率进行拆解，把泛化误差分解为偏差、方差、噪声之和
* 偏差度量了学习算法的期望预测与真实结果的偏离程度，也就是学习算法本身的拟合能力

* 方差度量了同样大小的训练集的变动所导致的学习性能的的变化，刻画了数据如扰动所造成的影响

* 噪声表达了当前任务上任何学习算法所能达到的期望泛化误差的下界，刻画了学习问题本身的难度

> yD在里边代表x在数据集中的标记？？ y-yD代表了噪声

一般而言方差和偏差有冲突  从方差和偏差的角度解释了欠拟合和过拟合 p46  fig. 2.9

